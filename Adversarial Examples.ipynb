{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# reshape and normalize data\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "\n",
    "# convert labels to integer\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "# split dataset into train and validation set\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self):\n",
    "        # input and output shapes\n",
    "#         self.n_inputs = 28*28 # shape of MNIST images\n",
    "        self.n_hidden1 = 300 # 1st layer\n",
    "        self.n_hidden2 = 100 # 2nd layer\n",
    "        self.n_outputs = 10 # output layer, 10 digits\n",
    "\n",
    "        # hyperparameters\n",
    "        self.n_epochs = 40\n",
    "        self.batch_size = 50\n",
    "        self.learning_rate = 0.01\n",
    "\n",
    "        self.model_name = \"dnn-mnist\"\n",
    "    \n",
    "    def dense_layer(self, X, size, name, activation=None):\n",
    "        \"\"\"method that encapsulates a single layer of a feedforward neural network\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            input_size = int(X.get_shape()[1])\n",
    "            # sample according to xavier weight initialization technique\n",
    "            xavier_initialization = tf.truncated_normal((input_size, size), stddev=2/np.sqrt(input_size))\n",
    "            W = tf.Variable(xavier_initialization, dtype=tf.float32, name=\"weight\")\n",
    "            b = tf.Variable(np.zeros([size]), dtype=tf.float32, name=\"bias\")\n",
    "            z = tf.matmul(X, W) + b\n",
    "        if activation == \"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z\n",
    "    \n",
    "    def mlp(self, X):\n",
    "        with tf.name_scope(\"dnn\"):\n",
    "            hidden1 = self.dense_layer(X, self.n_hidden1, \"hidden1\", activation=\"relu\")\n",
    "            hidden2 = self.dense_layer(hidden1, self.n_hidden2, \"hidden2\", activation=\"relu\")\n",
    "            outputs = self.dense_layer(hidden2, self.n_outputs, \"outputs\")\n",
    "            logits = tf.nn.sigmoid(outputs, name=\"logits\")\n",
    "        \n",
    "        return outputs, logits\n",
    "    \n",
    "    def shuffle_batch(self, X, y):\n",
    "        rnd_idx = np.random.permutation(len(X))\n",
    "        n_batches = len(X) // self.batch_size\n",
    "        for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "            X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "            yield X_batch, y_batch\n",
    "    \n",
    "    def _infer(self, X):\n",
    "        output, logits = self.mlp(X)\n",
    "        \n",
    "        with tf.name_scope(\"prediction\"):\n",
    "            y_pred = tf.math.argmax(logits, axis=1)\n",
    "        return output, logits, y_pred\n",
    "    \n",
    "    def _loss_fn(self, logits, labels):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "            loss = tf.reduce_mean(cross_entropy, name=\"loss\")\n",
    "        return loss\n",
    "    \n",
    "    def _optimize_fn(self, loss):\n",
    "        with tf.name_scope(\"optimizer\"):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "            optimize_op = optimizer.minimize(loss)\n",
    "        return optimize_op\n",
    "    \n",
    "    def _eval_fn(self, logits, y):\n",
    "        with tf.name_scope(\"evaluate\"):\n",
    "            # tp: images that we predicted correctly\n",
    "            tp = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tp, tf.float32))\n",
    "            accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "        return accuracy\n",
    "    \n",
    "    def _train(self, X, y):\n",
    "        output, predicted_proba, predicted_label = self._infer(X)\n",
    "        loss = self._loss_fn(output, y)\n",
    "        optimize_op = self._optimize_fn(loss)\n",
    "        \n",
    "        loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "        \n",
    "        return optimize_op, loss, predicted_proba\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        tf.reset_default_graph()\n",
    "        input_dim = X_train.shape[1]\n",
    "        output_dim = y_train.shape[0]\n",
    "        \n",
    "        X = tf.placeholder(dtype=tf.float32, shape=(None, input_dim), name=\"X\")\n",
    "        y = tf.placeholder(dtype=tf.int32, shape=(None,), name=\"y\")\n",
    "        \n",
    "        optimize_op, loss, predicted_proba = self._train(X, y)\n",
    "        accuracy = self._eval_fn(predicted_proba, y)\n",
    "        \n",
    "        summary_op = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            train_writer = tf.summary.FileWriter(\"tf_logs/{model}/train_summary\".format(model=self.model_name), sess.graph)\n",
    "            valid_writer = tf.summary.FileWriter(\"tf_logs/{model}/valid_summary\".format(model=self.model_name), sess.graph)\n",
    "            for epoch in range(self.n_epochs):\n",
    "                for i, (X_batch, y_batch) in enumerate(self.shuffle_batch(X_train, y_train)):\n",
    "                    _, train_summary = sess.run([optimize_op, summary_op], feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "                train_writer.add_summary(train_summary, epoch)\n",
    "                accuracy_val, loss_val, valid_summary = sess.run([accuracy, loss, summary_op], feed_dict={X: X_valid, y: y_valid})\n",
    "                valid_writer.add_summary(valid_summary, epoch)\n",
    "\n",
    "                print(epoch, \"Validation accuracy:\", accuracy_val, \", Validaton Loss:\", loss_val)\n",
    "\n",
    "            save_path = saver.save(sess, \"tf_logs/{model}/model.ckpt\".format(model=self.model_name))\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "\n",
    "        input_dim = X_test.shape[1]\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        X = tf.placeholder(shape=[None, input_dim], dtype=tf.float32, name=\"input\")\n",
    "        \n",
    "        _, logits, preds = self._infer(X)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess, \"tf_logs/{model}/model.ckpt\".format(model=self.model_name))\n",
    "\n",
    "            # Result on test set batch.\n",
    "            logits_test, preds_test = sess.run([logits, preds], {X: X_test})\n",
    "\n",
    "        return preds_test, logits_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10)\n",
      "(?, 10)\n",
      "0 Validation accuracy: 0.9162 , Validaton Loss: 0.30630141\n",
      "1 Validation accuracy: 0.9352 , Validaton Loss: 0.23913532\n",
      "2 Validation accuracy: 0.9454 , Validaton Loss: 0.20411281\n",
      "3 Validation accuracy: 0.9502 , Validaton Loss: 0.181509\n",
      "4 Validation accuracy: 0.9546 , Validaton Loss: 0.16670328\n",
      "5 Validation accuracy: 0.9602 , Validaton Loss: 0.15294836\n",
      "6 Validation accuracy: 0.962 , Validaton Loss: 0.1424138\n",
      "7 Validation accuracy: 0.963 , Validaton Loss: 0.13676895\n",
      "8 Validation accuracy: 0.9654 , Validaton Loss: 0.12677358\n",
      "9 Validation accuracy: 0.967 , Validaton Loss: 0.12136215\n",
      "10 Validation accuracy: 0.969 , Validaton Loss: 0.11583159\n",
      "11 Validation accuracy: 0.969 , Validaton Loss: 0.11216262\n",
      "12 Validation accuracy: 0.9702 , Validaton Loss: 0.1057509\n",
      "13 Validation accuracy: 0.971 , Validaton Loss: 0.104279906\n",
      "14 Validation accuracy: 0.9706 , Validaton Loss: 0.099683866\n",
      "15 Validation accuracy: 0.9716 , Validaton Loss: 0.09761627\n",
      "16 Validation accuracy: 0.9714 , Validaton Loss: 0.09531978\n",
      "17 Validation accuracy: 0.9722 , Validaton Loss: 0.09196525\n",
      "18 Validation accuracy: 0.9732 , Validaton Loss: 0.09151837\n",
      "19 Validation accuracy: 0.972 , Validaton Loss: 0.08877485\n",
      "20 Validation accuracy: 0.9742 , Validaton Loss: 0.08732685\n",
      "21 Validation accuracy: 0.9754 , Validaton Loss: 0.08599972\n",
      "22 Validation accuracy: 0.9758 , Validaton Loss: 0.08410509\n",
      "23 Validation accuracy: 0.9762 , Validaton Loss: 0.08287928\n",
      "24 Validation accuracy: 0.9768 , Validaton Loss: 0.082407065\n",
      "25 Validation accuracy: 0.9766 , Validaton Loss: 0.081463814\n",
      "26 Validation accuracy: 0.9774 , Validaton Loss: 0.0810289\n",
      "27 Validation accuracy: 0.978 , Validaton Loss: 0.07977685\n",
      "28 Validation accuracy: 0.9782 , Validaton Loss: 0.07680344\n",
      "29 Validation accuracy: 0.9774 , Validaton Loss: 0.07579658\n",
      "30 Validation accuracy: 0.9786 , Validaton Loss: 0.075811915\n",
      "31 Validation accuracy: 0.9796 , Validaton Loss: 0.07641806\n",
      "32 Validation accuracy: 0.9792 , Validaton Loss: 0.073846415\n",
      "33 Validation accuracy: 0.9784 , Validaton Loss: 0.07441737\n",
      "34 Validation accuracy: 0.9802 , Validaton Loss: 0.072978824\n",
      "35 Validation accuracy: 0.9804 , Validaton Loss: 0.072961986\n",
      "36 Validation accuracy: 0.979 , Validaton Loss: 0.072792396\n",
      "37 Validation accuracy: 0.9794 , Validaton Loss: 0.07191712\n",
      "38 Validation accuracy: 0.9794 , Validaton Loss: 0.071739584\n",
      "39 Validation accuracy: 0.98 , Validaton Loss: 0.07111897\n"
     ]
    }
   ],
   "source": [
    "DNN().fit(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tf_logs/dnn-mnist/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1]),\n",
       " array([[0.00223768, 0.99999905, 0.77370834, 0.87840426, 0.34380037,\n",
       "         0.02013161, 0.03771162, 0.94163543, 0.9884288 , 0.12480121]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = np.random.randint(X_test.shape[0])\n",
    "image = X_test[c]\n",
    "plt.imshow(image.reshape((28,28)), cmap='Greys')\n",
    "DNN().predict(image.reshape(1,784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, (None, 28*28))\n",
    "X_hat = tf.Variable(np.zeros([1, 28*28]), dtype=tf.float32, name=\"X_hat\")\n",
    "assign_op = tf.assign(X_hat, X)\n",
    "y_hat = tf.placeholder(tf.int32, (None,))\n",
    "\n",
    "learning_rate = 0.01\n",
    "# labels = tf.reshape(tf.one_hot(y_hat, 10), (10,))\n",
    "output, logits, _ = DNN()._infer(X_hat)\n",
    "loss = DNN()._loss_fn(output, y_hat)\n",
    "\n",
    "# loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_hat, logits=output)\n",
    "optimize_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, var_list=[X_hat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'X_hat:0' shape=(1, 784) dtype=float32_ref>,\n",
       " <tf.Variable 'dnn/hidden1/weight:0' shape=(784, 300) dtype=float32_ref>,\n",
       " <tf.Variable 'dnn/hidden1/bias:0' shape=(300,) dtype=float32_ref>,\n",
       " <tf.Variable 'dnn/hidden2/weight:0' shape=(300, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'dnn/hidden2/bias:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'dnn/outputs/weight:0' shape=(100, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dnn/outputs/bias:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.5959819555282593\n",
      "loss=0.28551343083381653\n",
      "loss=0.1584867238998413\n",
      "loss=0.10962539166212082\n",
      "loss=0.08399546146392822\n",
      "loss=0.06811459362506866\n",
      "loss=0.05723772570490837\n",
      "loss=0.04944736883044243\n",
      "loss=0.043569568544626236\n",
      "loss=0.03903026878833771\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(assign_op, feed_dict={X: training_example})\n",
    "\n",
    "    for i in range(1000):\n",
    "        _, loss_value = sess.run([optimize_op, loss], feed_dict={y_hat: [4]})\n",
    "        if i % 100 == 0:\n",
    "            print('loss={}'.format(loss_value))\n",
    "    adv = X_hat.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tf_logs/dnn-mnist/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([7]),\n",
       " array([[3.4682870e-01, 5.4198904e-03, 9.5374697e-01, 9.9063206e-01,\n",
       "         1.4025898e-03, 2.4133573e-01, 3.4396802e-05, 9.9999499e-01,\n",
       "         2.1551281e-01, 8.5841882e-01]], dtype=float32))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNN().predict(X_test[0].reshape(1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tf_logs/dnn-mnist/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([7]),\n",
       " array([[5.97642422e-01, 8.44278838e-03, 9.84283805e-01, 9.97051120e-01,\n",
       "         4.26000275e-04, 2.09219292e-01, 1.20509385e-05, 9.99996662e-01,\n",
       "         1.28022730e-01, 8.27773571e-01]], dtype=float32))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNN().predict(adv.reshape(1,784))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
